
model_for: grpo
model: gpt2
dtype: bfloat16

ckpt: saved_ckpt/sft_gpt2_fsdp_900_1740418880_length.pt
ckpt_type: sft

batch_size: 256
gradient_step: 16
lr: 0.000005
min_lr: 0.000001
epoch: 3

n_samples: 64
n_samples_select: 8

temperature: 0.7
top_p: 1.0

max_prompt_length: 384
max_response_length: 56

#noptepochs: 2  # number of ppo iterations
penalty_reward_value: -1  # if a sampling response has no end token, set its reward to this value
beta: 0.1
label_smoothing: 0.0

cliprange: 0.2
kl_coef: 0.03

normalize_sample: 1024
normalize_batch_size: 64

eval_sample: 1024
eval_batch_size: 64

activation_checkpointing: True

checkpoint_type: FULL_STATE_DICT # SHARDED_STATE_DICT, or FULL_STATE_DICT
checkpoint_folder: checkpoint
model_save_name: reward_gpt2
dist_checkpoint_root_folder: checkpoint
dist_checkpoint_folder: shard
save_using_num_threads: 4

policy_ref:
  model: gpt2
  ckpt: saved_ckpt/sft_gpt2_fsdp_900_1740418880_length.pt
  ckpt_type: sft

reward_model:
  model: gpt2-large
  ckpt: saved_ckpt/reward_gpt2-large_fsdp_1100_1740431466_length.pt
  ckpt_type: reward
