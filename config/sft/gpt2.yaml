model_for: sft
model: gpt2
dtype: bfloat16

max_prompt_length: 384
max_response_length: 56

batch_size: 64
gradient_step: 1  # gradient accumulation steps
max_gradient: 1
lr: 0.00007
min_lr: 0.00001
epoch: 1

eval_batch_size: 32
eval_sample: 1024

activation_checkpointing: False

checkpoint_type: FULL_STATE_DICT # SHARDED_STATE_DICT, or FULL_STATE_DICT
checkpoint_folder: checkpoint
model_save_name: sft_gpt2
dist_checkpoint_root_folder: checkpoint
dist_checkpoint_folder: shard
save_using_num_threads: 4
